# **Operational Characteristics of SSDs in Enterprise Storage Systems: A Large-Scale Field Study**

这篇论文由多伦多大学和NetApp联合发表在FAST 2022上，作者是Stathis Maneas。这篇论文统计分析了NetApp的近两百万块SSD在过去4年里的运行数据，以此总结了 SSD 寿命，写放大这两个核心运维特性在大规模线上环境下的真实情况及其影响因素。

## Introduction

本文基于NetApp的企业存储系统，首次对NAND SSD的几个关键特性进行了大规模的真实研究。本文的研究基于4年多来收集的NetApp SSD总体样本的遥测数据，涵盖总计超过10亿块硬盘。具体来说，我们研究的SSD包括近200万个驱动器，涵盖3个制造商，20个不同的系列，2个接口(SAS和NVMe)，以及4种主要的闪存技术，即cMLC(消费级)，eMLC(企业级)，3D-TLC和3D-eTLC。我们的数据集非常丰富，包括关于使用情况的信息，例如主机读写、物理设备写入总量，以及关于每个驱动器的磨损水平和写入放大的信息。

论文想要回答的核心问题包括：

1. 实际生产环境中真实的写入速率到底有多快？SSD 的寿命能有多长？未来 QLC 寿命会更短，它能否满足数据中心及企业级应用的需求？
2. 实际生产环境中的 SSD 写放大会有多严重？之前学术界的研究分析是否符合实际情况？
3. 实际生产环境中 SSD 的磨损均衡做得好不好？
4. SSD 的写放大从理论上分析会和很多因素有关，那在实际生产环境中，这些因素的影响到底有多大呢？哪些因素才是主要的呢？

## Methodology

下表展示了数据集中硬盘的不同厂商、系列及其容量、P/E次数、DWPD（Diskful Write Per Day）、OP（预留空间）等，除最后 II-X, II-Y, II-Z 为 NVME 接口外其余 SSD 均为 SAS 接口的，可以推测从数量上看 SAS SSD 也占了绝大部分。不过 SSD 的接口类型对其寿命，写放大应该是没有什么影响的，故尽管未来 NVME SSD 会越来越多，这篇论文依然会有参考价值。

![Table 1](https://img.gaomf.cn/202204072036558.png)

根据用途，系统分为两种不同类型：

- 作为 HDD 盘的高速缓存层使用，提供低读取延迟和增加系统吞吐量，简称为 **WBC** 应用（Write-Back Cache）
- 直接构成全 SSD 阵列，简称为 **AFF** 应用（All Flash Fabric-attached-storage）

这两类系统的读写特性差异较大，因此论文中基本都是将其分开进行讨论。

## SSD整体使用情况

### NAND使用率

SSD有个重要指标是 P/E Cycle Limit，即最大擦写循环次数（Program-Erase Cycle），这是决定 SSD 寿命的根本性因素，SSD 寿命预测基本就是根据当前 P/E Cycle 与最大 P/E Cycle Limit 的比值得出的。因此作者引入了一个年损耗率的概念：
$$
Ann.NAND\ \ Usage\ Rate =\frac{\%of\ PE\ Cycle\ Limit\ Used\ So\ Far}{Power-On\  Years}
$$
显而易见，此值的含义就是每年会用去 SSD 总寿命的百分比。实际统计结果如下：

![Figure 2](https://img.gaomf.cn/202204072207243.png)

由图可知：

- 绝大部分 SSD 年损耗率其实是非常低的，AFF 应用中 60% 的 SSD 年损耗率小于 1%，也就是这部分 SSD 用 100 年才会报废
- 尽管 WBC 应用的写入量比 AFF 应用要高不少，然而二者的年损耗率其实是差不多的，原因是 WBC 应用的写放大系数要低于 AFF 应用，这抵消了其写入量更高的影响
- 奇怪的是， I-C，I-D，I-E，I-B 这几个型号的 SSD 有着远超其他 SSD 的年损耗率，然而其写入数据数量并没有和其他 SSD 有明显区别。因此猜测其高NAND使用率是由于垃圾回收、磨损均衡等机制。

根据以上数据我们可以来评估下未来 QLC 的寿命是否能满足数据中心及企业级应用的需求了，这在论文作者的 PPT 里面有计算结果：

![QLC Projection](https://img.gaomf.cn/202204072221591.png)

- 未来 QLC SSD 的 P/E Cycle Limit 预计在 1k ~ 3k 之间
- 如果按 5 年寿命计算，约有 86% ~ 95% 的盘可以用 QLC 替代
- 如果按 7 年寿命计算，约有 82% ~ 92% 的盘可以用 QLC 替代
- 最终结论：系统可以整体迁移至 QLC 上

### 写放大系数分析

写放大系数（Write Amplification Factor, **WAF**）是影响 SSD 寿命及性能的关键指标，SSD 存在写放大的原因是其内部存在诸多背景任务（housekeeping tasks），如垃圾回收（GC），磨损均衡（Wear-Leveling）等。学术界和产业界对于如何控制写放大这一问题都做了很多工作，然而实际 SSD 产品这一点做得究竟如何呢？已有的统计研究都不够广泛，规模也相对较小，因此这篇论文的研究就显得比较有价值了。下面就来看下作者的统计结果吧：

![Figure 3](https://img.gaomf.cn/202204081102721.png)

- 此研究中实际观察到的 WAF 比之前已有研究中观察到的（1.x 左右）要高得多，96% 的 SSD WAF 都超过了 1.5，中位数达到了 6 左右
- WAF 的分布范围很广，10% 分位值仅为 2，中位数 6，然而 99% 分位值达到了 480。这说明不同应用和设备的 WAF 差异性很大。WAF 的影响因素将在下文详细分析
- 可以看到I-C、I-D、I-E具有远高于其他系列的WAF，中位数竟然就达到了 100 左右，经过检查发现，这些SSD在空闲时频繁对block进行重写来防止数据丢失，因为超过了ECC可以纠正的错误。

**应用负载与SSD的关系：**

- 同一个型号的 SSD 也表现出了很大的 WAF 差异，其 95% 分位值是中位数的 9 倍以上。同一型号的盘会被用在不同应用上，因此合理的解释就是不同应用负载情况下 WAF 会表现出很大的差异性
- WBC 应用的 WAF 比 AFF 应用要小 ，这也就是前文观察到的，尽管 WBC 写入量要高更多，然而二者实际 SSD 损耗速度差不多的原因了。因此可以说 WBC 应用在某种程度上对 SSD 更为友好

**与模拟研究相比**

学术界在研究 SSD WAF 时往往使用仿真的方法，然而根据本文的结论，这些仿真得到的 WAF 都太小了，绝大部分已有理论研究论文中 WAF 最高也就 10 左右，这仅仅只是 NetApp 真实环境中的 60% 分位值。作者认为造成这一差异的主要原因有：

- 理论研究所使用的 IO Trace 数据太古老了，已经没法反映当今实际运行在 SSD 上的应用负载。此外古老的 IO Trace 数据很多是基于 HDD 盘抓取的，不会存在 TRIM 等 SSD 特有命令
- SSD 仿真软件（如 FEMU）对 FTL 固件的行为仿真存在很大困难，很有可能使用的只是理想模型，这与实际行为差异会很大，且 FTL 固件每家厂商，甚至是不同型号都是有区别的，这让仿真的准确度变得更低了

### 磨损均衡分析

磨损均衡（Wear-Leveling）就是把写入尽量平均的打散到所有 Block 上，以此避免某些 Block 被频繁的擦写，造成其寿命下降及性能降低。磨损均衡技术是以整体的 P/E Cycle 增加来换取长尾减少的，因此实际实现上需要做一个平衡——太过激进的磨损均衡会导致 SSD 整体寿命衰减得太快。为了衡量磨损均衡的效果，作者引入了擦除率（Erase Ratio）及擦除均匀度（Erase Difference）这两个指标：

![](https://img.gaomf.cn/202204081437256.png)

![Figure 4](https://img.gaomf.cn/202204081441488.png?800x)

- Erase Ratio 的中位数是 1.55，意味着写入量最多的 Block 比平均值多写入了 55% 的数据
- 5% 的设备 Erase Ratio 大于 6 了，这意味着部分 Block 可能会在全盘只用了 16% PE Cycle 的时候就报废了，这会对 SSD 的整体性能，特别是长尾表现造成明显影响
- 部分型号的 SSD 磨损均衡做得很好，十分接近理论值，说明这个要做好是可以做好的
- 然而，I-C和I-D依然表现出了极差的磨损均衡，在极高的WAF下并没能做到更好的磨损均衡。

## 写放大系数的影响因素

### 闪存转换层（FTL）

![Which factors impact WAF?](https://img.gaomf.cn/202204081457878.png?800x)

结论很简单，**FTL 固件算法做得如何是决定性因素；写入负载也有明显影响；其余因素基本没影响。**

作者此处选用了 III-A 这款 SSD 为例进行分析，这款 SSD 最初的固件版本是 FV2，后面升级到了 FV3：

![Figure 6](https://img.gaomf.cn/202204081510216.png?500x)

可以看到这次纯软件固件升级有显著优化作用，明显改善了其 WAF；由此可见，SSD 远远不只是搞一些 Flash 颗粒芯片来组装下就好了的，主控软件算法的水平同样是决定性的；甚至可以说，主控的影响有时候比颗粒类型更重要。

### 负载特性

直接进行 IO-Trace 是记录负载特性最有效的方法，然而对于大规模系统来说这是基本不可行的。因此作者用 DWPD，SSD 容量，SSD 接口类型这几个指标来对负载类型进行了一个简单分类。SSD 容量和接口类型能用于区分负载类型同样是由于不同容量和接口类型的 SSD 被用于了不同产品和客户上。

结论是：

- 越高的 DWPD 会对应越低的 WAF。可能的原因是， SSD 在更高的写入速率下运行效率更高，也可能意味着很多 FTL 固件实现上是以固定的频率在进行后台任务的，即不强烈依赖于 DWPD，因此较高的 DWPD 将减少这种恒定工作对 WAF 比率的影响。
- 还能发现较大容量的 SSD WAF 相对也较小一些，但并不是主要因素，其他因素对 WAF 的影响更大
- NVME 接口的 WAF 要小于 SAS，这个更有可能的原因是，NVME 接口更新，使用 NVME 的业务也会更新一些，也就会更多的考虑 SSD 的特性进行优化

### 空间使用率

无明显影响

### 预留空间

预留空间（OP） 是指保留作为备用容量的驱动器容量的一部分，以提高驱动器的磨损均衡、垃圾收集和随机写入性能；因此，预计将有助于减少 WAF。然而实际看下来影响不大，甚至是有轻微的负相关关系，即预留空间越大的 SSD 反而 WAF 也越大。这表明可能还有其他因素（例如，工作负载特征和固件）比 OP 的影响更大。

### 多流写

多流写（Multi-stream Writes, MSW）技术需要主机端在写入的时候指定一个 Stream ID，支持此特性的 SSD 会根据此 Stream ID 将具有相同或相似生命周期的数据写入到相同的 Block 中去，以此实现冷热数据分离，预期可以大幅提高 GC 时的效率，减少 WAF。作者对多流写技术的影响结论是不确定，原因是缺乏足够的数据进行判断。

## 读写比例

![Figure 9](https://img.gaomf.cn/202204072123744.png)

我们观察到：

- 绝大多数驱动器（大约 94%）的读取次数多于写入次数，这个读写比例与基于 HDD 的存储系统的跟踪分析结果形成鲜明对比，后者通常写入多于读取。
- 可以看到 WBC 应用的读写比例比 AFF 应用更高，对于 WBC 应用来说，更高的读写比是期望中的，这也就意味着更高的 Cache 使用率

Facebook，Microsoft，Alibaba 的统计数据中也是读比写多，不过 NetApp 这次公开的统计数据读写量远高于其余几家公开的统计数据。可能由于NetApp统计的是主机写入，而数据中心统计的是NAND物理写入，同时也突出了企业存储系统和数据中心之间的差异（在工作负载特征方面）。

![Figure 11](https://img.gaomf.cn/202204072132446.png)
